{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/flatfeetpete/nerfstudio/blob/main/colab/demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiiXJ7K_fePG"
      },
      "source": [
        "<p align=\"center\">\n",
        "    <picture>\n",
        "    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://docs.nerf.studio/en/latest/_images/logo-dark.png\">\n",
        "    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://docs.nerf.studio/en/latest/_images/logo.png\">\n",
        "    <img alt=\"nerfstudio\" src=\"https://docs.nerf.studio/en/latest/_images/logo.png\" width=\"400\">\n",
        "    </picture>\n",
        "</p>\n",
        "\n",
        "\n",
        "# Nerfstudio: A collaboration friendly studio for NeRFs\n",
        "\n",
        "\n",
        "![GitHub stars](https://img.shields.io/github/stars/nerfstudio-project/nerfstudio?color=gold&style=social)\n",
        "\n",
        "This colab shows how to train and view NeRFs from Nerfstudio both on pre-made datasets or from your own videos/images.\n",
        "\n",
        "\\\\\n",
        "\n",
        "Credit to [NeX](https://nex-mpi.github.io/) for Google Colab format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yyx5h6kz5ga7"
      },
      "source": [
        "## Frequently Asked Questions\n",
        "\n",
        "*  **Downloading custom data is stalling (no output):**\n",
        "    * This is a bug in Colab. The data is processing, but may take a while to complete. You will know processing completed if `data/nerfstudio/custom_data/transforms.json` exists. Terminating the cell early will result in not being able to train.\n",
        "*  **Processing custom data is taking a long time:**\n",
        "    * The time it takes to process data depends on the number of images and its resolution. If processing is taking too long, try lowering the resolution of your custom data.\n",
        "*  **Error: Data processing did not complete:**\n",
        "    * This means that the data processing script did not fully complete. This could be because there were not enough images, or that the images were of low quality. We recommend images with little to no motion blur and lots of visual overlap of the scene to increase the chances of successful processing.\n",
        "*   **Training is not showing progress**:\n",
        "    * The lack of output is a bug in Colab. You can see the training progress from the viewer.\n",
        "* **Viewer Quality is bad / Low resolution**:\n",
        "    * This may be because more GPU is being used on training that rendering the viewer. Try pausing training or decreasing training utilization.\n",
        "* **WARNING: Running pip as the 'root' user...:**:\n",
        "    * This and other pip warnings or errors can be safely ignored.\n",
        "* **Other problems?**\n",
        "    * Feel free to create an issue on our [GitHub repo](https://github.com/nerfstudio-project/nerfstudio).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGr33zHaHak0",
        "outputId": "d688b52c-badc-403d-e809-238f2410e44b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mâœ¨ğŸ°âœ¨ Everything looks OK!\n"
          ]
        }
      ],
      "source": [
        "#@markdown <h1>Install Conda (requires runtime restart)</h1>\n",
        "\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oyLHl8QfYwP",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown <h1>Install Nerfstudio and Dependencies (~15 min)</h1>\n",
        "\n",
        "%cd /content/\n",
        "!pip install --upgrade pip\n",
        "!pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# Installing TinyCuda\n",
        "%cd /content/\n",
        "!gdown \"https://drive.google.com/u/1/uc?id=12RL_NVgE9WGvr_fEsXEiuaJ1QESvQCPl&confirm=t\" \n",
        "!pip install tinycudann-1.7-cp38-cp38-linux_x86_64.whl\n",
        "\n",
        "# Installing COLMAP\n",
        "%cd /content/\n",
        "!conda install -c conda-forge colmap\n",
        "\n",
        "# Install nerfstudio\n",
        "%cd /content/\n",
        "# !pip install nerfstudio\n",
        "!pip install git+https://github.com/nerfstudio-project/nerfstudio.git\n",
        "!conda remove --force qt-main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "msVLprI4gRA4",
        "outputId": "3a383cd1-f5e3-40eb-eb41-3c04af7a9b7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "--2023-02-26 20:21:20--  https://data.nerf.studio/nerfstudio/dozer.zip\n",
            "Resolving data.nerf.studio (data.nerf.studio)... 34.102.68.79\n",
            "Connecting to data.nerf.studio (data.nerf.studio)|34.102.68.79|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1347483375 (1.3G) [application/zip]\n",
            "Saving to: â€˜data/nerfstudio/dozer.zipâ€™\n",
            "\n",
            "data/nerfstudio/doz 100%[===================>]   1.25G  1.74MB/s    in 4m 13s  \n",
            "\n",
            "2023-02-26 20:25:34 (5.08 MB/s) - â€˜data/nerfstudio/dozer.zipâ€™ saved [1347483375/1347483375]\n",
            "\n",
            "\u001b[0mData Processing Succeeded!\n"
          ]
        }
      ],
      "source": [
        "#@markdown <h1> Downloading and Processing Data</h1>\n",
        "#@markdown <h3>Pick the preset scene or upload your own images/video</h3>\n",
        "import os\n",
        "import glob\n",
        "from google.colab import files\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "scene = '\\uD83D\\uDE9C dozer' #@param ['ğŸ–¼ poster', 'ğŸšœ dozer', 'ğŸŒ„ desolation', 'ğŸ“¤ upload your images' , 'ğŸ¥ upload your own video', 'ğŸ”º upload Polycam data', 'ğŸ’½ upload your own Record3D data']\n",
        "scene = ' '.join(scene.split(' ')[1:])\n",
        "\n",
        "if scene == \"upload Polycam data\":\n",
        "    %cd /content/\n",
        "    !mkdir -p /content/data/nerfstudio/custom_data\n",
        "    %cd /content/data/nerfstudio/custom_data/\n",
        "    uploaded = files.upload()\n",
        "    dir = os.getcwd()\n",
        "    if len(uploaded.keys()) > 1:\n",
        "        print(\"ERROR, upload a single .zip file when processing Polycam data\")\n",
        "    dataset_dir = [os.path.join(dir, f) for f in uploaded.keys()][0]\n",
        "    !ns-process-data polycam --data $dataset_dir --output-dir /content/data/nerfstudio/custom_data/\n",
        "    scene = \"custom_data\"\n",
        "elif scene == 'upload your own Record3D data':\n",
        "    display(HTML('<h3>Zip your Record3D folder, and upload.</h3>'))\n",
        "    display(HTML('<h3>More information on Record3D can be found <a href=\"https://docs.nerf.studio/en/latest/quickstart/custom_dataset.html#record3d-capture\" target=\"_blank\">here</a>.</h3>'))\n",
        "    %cd /content/\n",
        "    !mkdir -p /content/data/nerfstudio/custom_data\n",
        "    %cd /content/data/nerfstudio/custom_data/\n",
        "    uploaded = files.upload()\n",
        "    dir = os.getcwd()\n",
        "    preupload_datasets = [os.path.join(dir, f) for f in uploaded.keys()]\n",
        "    record_3d_zipfile = preupload_datasets[0]\n",
        "    !unzip $record_3d_zipfile -d /content/data/nerfstudio/custom_data\n",
        "    custom_data_directory = glob.glob('/content/data/nerfstudio/custom_data/*')[0]\n",
        "    !ns-process-data record3d --data $custom_data_directory --output-dir /content/data/nerfstudio/custom_data/\n",
        "    scene = \"custom_data\"\n",
        "elif scene in ['upload your images', 'upload your own video']:\n",
        "    display(HTML('<h3>Select your custom data</h3>'))\n",
        "    display(HTML('<p/>You can select multiple images by pressing ctrl, cmd or shift and click.<p>'))\n",
        "    display(HTML('<p/>Note: This may take time, especially on hires inputs, so we recommend to download dataset after creation.<p>'))\n",
        "    !mkdir -p /content/data/nerfstudio/custom_data\n",
        "    if scene == 'upload your images':\n",
        "        !mkdir -p /content/data/nerfstudio/custom_data/raw_images\n",
        "        %cd /content/data/nerfstudio/custom_data/raw_images\n",
        "        uploaded = files.upload()\n",
        "        dir = os.getcwd()\n",
        "    else:\n",
        "        %cd /content/data/nerfstudio/custom_data/\n",
        "        uploaded = files.upload()\n",
        "        dir = os.getcwd()\n",
        "    preupload_datasets = [os.path.join(dir, f) for f in uploaded.keys()]\n",
        "    del uploaded\n",
        "    %cd /content/\n",
        "\n",
        "    if scene == 'upload your images':\n",
        "        !ns-process-data images --data /content/data/nerfstudio/custom_data/raw_images --output-dir /content/data/nerfstudio/custom_data/\n",
        "    else:\n",
        "        video_path = preupload_datasets[0]\n",
        "        !ns-process-data video --data $video_path --output-dir /content/data/nerfstudio/custom_data/\n",
        "\n",
        "    scene = \"custom_data\"\n",
        "else:\n",
        "    %cd /content/\n",
        "    !ns-download-data nerfstudio --capture-name=$scene\n",
        "\n",
        "print(\"Data Processing Succeeded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 950
        },
        "id": "VoKDxqEcjmfC",
        "outputId": "4b5194fc-65e2-4fd0-b58e-9fee9e728cd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "\u001b[K\u001b[?25h/tools/node/bin/lt -> /tools/node/lib/node_modules/localtunnel/bin/lt.js\n",
            "+ localtunnel@2.0.2\n",
            "updated 1 package in 1.029s\n",
            "https://viewer.nerf.studio/?websocket_url=wss://rude-shrimps-suffer-34-70-201-170.loca.lt\n",
            "You may need to click Refresh Page after you start training!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7f6bd0b3c370>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"100%\"\n",
              "            height=\"800\"\n",
              "            src=\"https://viewer.nerf.studio/?websocket_url=wss://rude-shrimps-suffer-34-70-201-170.loca.lt\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "#@markdown <h1>Set up and Start Viewer</h1>\n",
        "\n",
        "%cd /content\n",
        "\n",
        "# Install localtunnel\n",
        "# We are using localtunnel https://github.com/localtunnel/localtunnel but ngrok could also be used\n",
        "!npm install -g localtunnel\n",
        "\n",
        "# Tunnel port 7007, the default for\n",
        "!rm url.txt 2> /dev/null\n",
        "get_ipython().system_raw('lt --port 7007 >> url.txt 2>&1 &')\n",
        "\n",
        "import time\n",
        "time.sleep(3) # the previous command needs time to write to url.txt\n",
        "\n",
        "\n",
        "with open('url.txt') as f:\n",
        "  lines = f.readlines()\n",
        "websocket_url = lines[0].split(\": \")[1].strip().replace(\"https\", \"wss\")\n",
        "# from nerfstudio.utils.io import load_from_json\n",
        "# from pathlib import Path\n",
        "# json_filename = \"nerfstudio/nerfstudio/viewer/app/package.json\"\n",
        "# version = load_from_json(Path(json_filename))[\"version\"]\n",
        "url = f\"https://viewer.nerf.studio/?websocket_url={websocket_url}\"\n",
        "print(url)\n",
        "print(\"You may need to click Refresh Page after you start training!\")\n",
        "from IPython import display\n",
        "display.IFrame(src=url, height=800, width=\"100%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "m_N8_cLfjoXD"
      },
      "outputs": [],
      "source": [
        "#@markdown <h1>Start Training</h1>\n",
        "\n",
        "%cd /content\n",
        "if os.path.exists(f\"data/nerfstudio/{scene}/transforms.json\"):\n",
        "    !ns-train nerfacto --viewer.websocket-port 7007 nerfstudio-data --data data/nerfstudio/$scene --downscale-factor 4\n",
        "else:\n",
        "    from IPython.core.display import display, HTML\n",
        "    display(HTML('<h3 style=\"color:red\">Error: Data processing did not complete</h3>'))\n",
        "    display(HTML('<h3>Please re-run `Downloading and Processing Data`, or view the FAQ for more info.</h3>'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "RnyMpY4xu_Av",
        "outputId": "cc9f4258-c0ef-48ea-a1ce-0a894a7980e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This restarts training, but only works if you finished training on your first run. You can use the UI to continue traiing if it didn't finish.\n",
        "%cd /content\n",
        "!echo $(ls -rt outputs/data-nerfstudio-$scene/nerfacto | head -1)\n",
        "!ns-train nerfacto --viewer.websocket-port 7007 --load-dir=outputs/data-nerfstudio-$scene/nerfacto/$(ls -rt outputs/data-nerfstudio-$scene/nerfacto | head -1)/nerfstudio_models --viewer.start-train=False nerfstudio-data --data data/nerfstudio/$scene --downscale-factor 4\n"
      ],
      "metadata": {
        "id": "7nl2vimeG80v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGt8ukG6Htg3",
        "outputId": "fa946890-c7d8-4e46-a54e-7231bc5a2059"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2;36m[19:48:48]\u001b[0m\u001b[2;36m \u001b[0mSkipping \u001b[1;36m0\u001b[0m files in dataset split train.                                          \u001b]8;id=527413;file:///content/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\u001b\\\u001b[2mnerfstudio_dataparser.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=243595;file:///content/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#91\u001b\\\u001b[2m91\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0mSkipping \u001b[1;36m0\u001b[0m files in dataset split test.                                           \u001b]8;id=109270;file:///content/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\u001b\\\u001b[2mnerfstudio_dataparser.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=464675;file:///content/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#91\u001b\\\u001b[2m91\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2KLoading data batch \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\u001b[2KLoading data batch \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h/usr/local/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Loading latest checkpoint from load_dir\n",
            "âœ… Done loading checkpoint from \n",
            "outputs/data-nerfstudio-poster/nerfacto/\u001b[1;36m2022\u001b[0m-\u001b[1;36m10\u001b[0m-29_192844/nerfstudio_models/step-\u001b[1;36m000014000.\u001b[0mckpt\n",
            "\u001b[1;32mCreating trajectory video\u001b[0m\n",
            "\u001b[2KğŸ¥ Rendering ğŸ¥ \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m100%\u001b[0m \u001b[31m0.14 fps\u001b[0m \u001b[33m11:47\u001b[0m\n",
            "\u001b[2K\u001b[32m(  â—   )\u001b[0m \u001b[33mSaving video\u001b[0m\n",
            "\u001b[1A\u001b[2K\u001b[92mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \u001b[0m\u001b[32m ğŸ‰ ğŸ‰ ğŸ‰ Success ğŸ‰ ğŸ‰ ğŸ‰\u001b[0m\u001b[92m â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
            "                                           \u001b[32mSaved video to renders/output.mp4\u001b[0m                                            \n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "#@title # Render Video { vertical-output: true }\n",
        "#@markdown <h3>Export the camera path from within the viewer, then run this cell.</h3>\n",
        "#@markdown <h5>The rendered video should be at renders/output.mp4!</h5>\n",
        "\n",
        "\n",
        "base_dir = \"/content/outputs/data-nerfstudio-\" + scene + \"/nerfacto/\"\n",
        "training_run_dir = base_dir + os.listdir(base_dir)[0]\n",
        "\n",
        "from IPython.core.display import display, HTML\n",
        "display(HTML('<h3>Upload the camera path JSON.</h3>'))\n",
        "%cd $training_run_dir\n",
        "uploaded = files.upload()\n",
        "uploaded_camera_path_filename = list(uploaded.keys())[0]\n",
        "\n",
        "config_filename = training_run_dir + \"/config.yml\"\n",
        "camera_path_filename = training_run_dir + \"/\" + uploaded_camera_path_filename\n",
        "camera_path_filename = camera_path_filename.replace(\" \", \"\\\\ \").replace(\"(\", \"\\\\(\").replace(\")\", \"\\\\)\")\n",
        "\n",
        "%cd /content/\n",
        "!ns-render --load-config $config_filename --traj filename --camera-path-filename $camera_path_filename --output-path renders/output.mp4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/nerf-work/\n",
        "!git pull\n",
        "%cd /content/nerfstudio/\n",
        "!git pull"
      ],
      "metadata": {
        "id": "dxXXq2lwTdhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/nerf-work/\n",
        "!git add --all\n",
        "!git commit -a -m 'dummy commit message (likely test files)'\n",
        "!git push"
      ],
      "metadata": {
        "id": "H4G42Uc68YS6",
        "outputId": "b2b75f88-78a8-40f4-89a7-15aed21835b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nerf-work\n",
            "[main 5929dba] dummy commit message (likely test files)\n",
            " 2 files changed, 1 insertion(+)\n",
            " create mode 100644 2023-02-28_2D-orbit.json\n",
            " create mode 100644 20230227_dozer_2d_sbs_orbit_ed0.05.mp4\n",
            "Enumerating objects: 5, done.\n",
            "Counting objects: 100% (5/5), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (4/4), done.\n",
            "Writing objects: 100% (4/4), 2.58 MiB | 8.94 MiB/s, done.\n",
            "Total 4 (delta 1), reused 0 (delta 0)\n",
            "remote: Resolving deltas: 100% (1/1), completed with 1 local object.\u001b[K\n",
            "To github.com:flatfeetpete/nerf-work.git\n",
            "   7fafb0e..5929dba  main -> main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "!export PYTHONPATH=/content/nerfstudio/:/env/python; python3 /content/nerfstudio/scripts/render.py --load-config /content/outputs/data-nerfstudio-dozer/nerfacto/2023-02-27_235450/config.yml --traj filename --camera-path-filename /content/nerf-work/2023-02-27-longer-path.json --output-path /content/nerf-work/20230227_dozer_square_sbs_65_ed0.05.mp4\n",
        "%cd /content/nerf-work/\n",
        "!git add --all\n",
        "!git commit -a -m 'dummy commit message (likely test files)'\n",
        "!git push"
      ],
      "metadata": {
        "outputId": "fb3f94b6-7812-4c87-bd01-8dc0219ce879",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2DoUJWydydX"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "\u001b[2;36m[04:11:06]\u001b[0m\u001b[2;36m \u001b[0mSkipping \u001b[1;36m0\u001b[0m files in dataset split train.                                         \u001b]8;id=748243;file:///content/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\u001b\\\u001b[2mnerfstudio_dataparser.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=142858;file:///content/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#165\u001b\\\u001b[2m165\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0mSkipping \u001b[1;36m0\u001b[0m files in dataset split test.                                          \u001b]8;id=10814;file:///content/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\u001b\\\u001b[2mnerfstudio_dataparser.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=952164;file:///content/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#165\u001b\\\u001b[2m165\u001b[0m\u001b]8;;\u001b\\\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nerfstudio/scripts/render.py\", line 345, in <module>\n",
            "    entrypoint()\n",
            "  File \"/content/nerfstudio/scripts/render.py\", line 341, in entrypoint\n",
            "    tyro.cli(RenderTrajectory).main()\n",
            "  File \"/content/nerfstudio/scripts/render.py\", line 286, in main\n",
            "    _, pipeline, _ = eval_setup(\n",
            "  File \"/content/nerfstudio/nerfstudio/utils/eval_utils.py\", line 100, in eval_setup\n",
            "    pipeline = config.pipeline.setup(device=device, test_mode=test_mode)\n",
            "  File \"/content/nerfstudio/nerfstudio/configs/base_config.py\", line 59, in setup\n",
            "    return self._target(self, **kwargs)\n",
            "  File \"/content/nerfstudio/nerfstudio/pipelines/base_pipeline.py\", line 237, in __init__\n",
            "    self._model = config.model.setup(\n",
            "  File \"/content/nerfstudio/nerfstudio/configs/base_config.py\", line 59, in setup\n",
            "    return self._target(self, **kwargs)\n",
            "  File \"/content/nerfstudio/nerfstudio/models/base_model.py\", line 82, in __init__\n",
            "    self.populate_modules()  # populate the modules\n",
            "  File \"/content/nerfstudio/nerfstudio/models/nerfacto.py\", line 226, in populate_modules\n",
            "    self.lpips = LearnedPerceptualImagePatchSimilarity(normalize=True)\n",
            "  File \"/usr/local/lib/python3.8/site-packages/torchmetrics/image/lpip.py\", line 125, in __init__\n",
            "    self.net = NoTrainLpips(net=net_type, verbose=False)\n",
            "  File \"/usr/local/lib/python3.8/site-packages/lpips/lpips.py\", line 84, in __init__\n",
            "    self.net = net_type(pretrained=not self.pnet_rand, requires_grad=self.pnet_tune)\n",
            "  File \"/usr/local/lib/python3.8/site-packages/lpips/pretrained_networks.py\", line 59, in __init__\n",
            "    alexnet_pretrained_features = tv.alexnet(pretrained=pretrained).features\n",
            "  File \"/usr/local/lib/python3.8/site-packages/torchvision/models/_utils.py\", line 142, in wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/site-packages/torchvision/models/_utils.py\", line 228, in inner_wrapper\n",
            "    return builder(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/site-packages/torchvision/models/alexnet.py\", line 112, in alexnet\n",
            "    model = AlexNet(**kwargs)\n",
            "  File \"/usr/local/lib/python3.8/site-packages/torchvision/models/alexnet.py\", line 44, in __init__\n",
            "    nn.Linear(4096, num_classes),\n",
            "  File \"/usr/local/lib/python3.8/site-packages/torch/nn/modules/linear.py\", line 101, in __init__\n",
            "    self.reset_parameters()\n",
            "  File \"/usr/local/lib/python3.8/site-packages/torch/nn/modules/linear.py\", line 107, in reset_parameters\n",
            "    init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
            "  File \"/usr/local/lib/python3.8/site-packages/torch/nn/init.py\", line 412, in kaiming_uniform_\n",
            "    return tensor.uniform_(-bound, bound)\n",
            "KeyboardInterrupt\n",
            "\u001b[0m^C\n",
            "/content/nerf-work\n",
            "[main 92cb0d8] dummy commit message (likely test files)\n",
            " 1 file changed, 0 insertions(+), 0 deletions(-)\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !ns-render --load-config outputs/data-nerfstudio-dozer/nerfacto/2023-02-28_010246/config.yml --traj filename --camera-path-filename data/nerfstudio/dozer/camera_paths/2023-02-28_010246.json --output-path renders/dozer/2023-02-28_010246.mp4\n",
        "#%cd /content\n",
        "#!cp data/nerfstudio/dozer/camera_paths/2023-02-28_010246.json nerf-work/longer_path.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gU8wSR7DmKeh",
        "outputId": "b71107fd-00f2-45a3-aa62-233db4e53eaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "# WORKED !ffmpeg -i /content/nerf-work/20230227_dozer_2d_sbs_orbit_ed0.05.mp4 -vcodec libx264 -x264opts \"frame-packing=3\" /content/nerf-work/20230227_dozer_2d_sbs_orbit_ed0.05.sbsembed.mp4\n",
        "\n",
        "!ffmpeg -stream_loop 6 -i /content/nerf-work/20230227_dozer_2d_sbs_orbit_ed0.05.mp4 -vcodec libx264 -x264opts \"frame-packing=3\" /content/nerf-work/20230227_dozer_.6x.sbs.mp4\n",
        "\n",
        "# FAILED  !ffmpeg -i /content/nerf-work/20230227_dozer_2d_sbs_orbit_ed0.05.mp4 -c copy -metadata:s:v:0 stereo_mode=1 /content/nerf-work/20230227_sbs001.mkv "
      ],
      "metadata": {
        "id": "ycWsbJ0wwQpc",
        "outputId": "f5fe1166-f08b-4b17-ed20-9b92b18b4cc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "ffmpeg version 4.2.7-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n",
            "  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-nvenc --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 31.100 / 56. 31.100\n",
            "  libavcodec     58. 54.100 / 58. 54.100\n",
            "  libavformat    58. 29.100 / 58. 29.100\n",
            "  libavdevice    58.  8.100 / 58.  8.100\n",
            "  libavfilter     7. 57.100 /  7. 57.100\n",
            "  libavresample   4.  0.  0 /  4.  0.  0\n",
            "  libswscale      5.  5.100 /  5.  5.100\n",
            "  libswresample   3.  5.100 /  3.  5.100\n",
            "  libpostproc    55.  5.100 / 55.  5.100\n",
            "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/content/nerf-work/20230227_dozer_2d_sbs_orbit_ed0.05.mp4':\n",
            "  Metadata:\n",
            "    major_brand     : isom\n",
            "    minor_version   : 512\n",
            "    compatible_brands: isomiso2avc1mp41\n",
            "    encoder         : Lavf58.29.100\n",
            "  Duration: 00:00:04.00, start: 0.000000, bitrate: 5379 kb/s\n",
            "    Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p, 1280x720, 5375 kb/s, 24 fps, 24 tbr, 12288 tbn, 48 tbc (default)\n",
            "    Metadata:\n",
            "      handler_name    : VideoHandler\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (h264 (native) -> h264 (libx264))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;36m[libx264 @ 0x55d0171f7280] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
            "\u001b[1;36m[libx264 @ 0x55d0171f7280] \u001b[0mprofile High, level 3.1\n",
            "\u001b[1;36m[libx264 @ 0x55d0171f7280] \u001b[0m264 - core 155 r2917 0a84d98 - H.264/MPEG-4 AVC codec - Copyleft 2003-2018 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=3 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=24 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 frame-packing=3 ip_ratio=1.40 aq=1:1.00\n",
            "Output #0, mp4, to '/content/nerf-work/20230227_dozer_.6x.sbs.mp4':\n",
            "  Metadata:\n",
            "    major_brand     : isom\n",
            "    minor_version   : 512\n",
            "    compatible_brands: isomiso2avc1mp41\n",
            "    encoder         : Lavf58.29.100\n",
            "    Stream #0:0(und): Video: h264 (libx264) (avc1 / 0x31637661), yuv420p, 1280x720, q=-1--1, 24 fps, 12288 tbn, 24 tbc (default)\n",
            "    Metadata:\n",
            "      handler_name    : VideoHandler\n",
            "      encoder         : Lavc58.54.100 libx264\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
            "frame=  672 fps=8.8 q=-1.0 Lsize=   17813kB time=00:00:27.87 bitrate=5234.9kbits/s speed=0.365x    \n",
            "video:17806kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.040037%\n",
            "\u001b[1;36m[libx264 @ 0x55d0171f7280] \u001b[0mframe I:8     Avg QP:22.63  size: 60343\n",
            "\u001b[1;36m[libx264 @ 0x55d0171f7280] \u001b[0mframe P:419   Avg QP:25.63  size: 32339\n",
            "\u001b[1;36m[libx264 @ 0x55d0171f7280] \u001b[0mframe B:245   Avg QP:27.49  size: 17141\n",
            "\u001b[1;36m[libx264 @ 0x55d0171f7280] \u001b[0mconsecutive B-frames: 39.6% 33.3%  6.2% 20.8%\n",
            "\u001b[1;36m[libx264 @ 0x55d0171f7280] \u001b[0mmb I  I16..4:  9.9% 76.0% 14.1%\n",
            "\u001b[1;36m[libx264 @ 0x55d0171f7280] \u001b[0mmb P  I16..4:  1.7% 19.1%  2.5%  P16..4: 38.7% 21.2% 11.3%  0.0%  0.0%    skip: 5.5%\n",
            "\u001b[1;36m[libx264 @ 0x55d0171f7280] \u001b[0mmb B  I16..4:  0.9%  5.3%  0.8%  B16..8: 48.5% 14.5%  4.5%  direct: 4.2%  skip:21.3%  L0:39.3% L1:45.5% BI:15.2%\n",
            "\u001b[1;36m[libx264 @ 0x55d0171f7280] \u001b[0m8x8 transform intra:80.7% inter:74.2%\n",
            "\u001b[1;36m[libx264 @ 0x55d0171f7280] \u001b[0mcoded y,uvDC,uvAC intra: 72.1% 74.1% 24.7% inter: 36.5% 28.7% 1.3%\n",
            "\u001b[1;36m[libx264 @ 0x55d0171f7280] \u001b[0mi16 v,h,dc,p: 27% 10%  8% 55%\n",
            "\u001b[1;36m[libx264 @ 0x55d0171f7280] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 27%  9% 14%  7%  8% 12%  5% 12%  6%\n",
            "\u001b[1;36m[libx264 @ 0x55d0171f7280] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 32%  7% 14%  7%  8% 12%  5% 11%  4%\n",
            "\u001b[1;36m[libx264 @ 0x55d0171f7280] \u001b[0mi8c dc,h,v,p: 52% 11% 29%  8%\n",
            "\u001b[1;36m[libx264 @ 0x55d0171f7280] \u001b[0mWeighted P-Frames: Y:3.3% UV:1.7%\n",
            "\u001b[1;36m[libx264 @ 0x55d0171f7280] \u001b[0mref P L0: 72.5% 22.8%  3.7%  1.0%  0.0%\n",
            "\u001b[1;36m[libx264 @ 0x55d0171f7280] \u001b[0mref B L0: 97.4%  2.1%  0.4%\n",
            "\u001b[1;36m[libx264 @ 0x55d0171f7280] \u001b[0mref B L1: 99.7%  0.3%\n",
            "\u001b[1;36m[libx264 @ 0x55d0171f7280] \u001b[0mkb/s:5209.30\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.13 ('nerfstudio')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "c59f626636933ef1dc834fb3684b382f705301c5306cf8436d2da634c2289783"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}